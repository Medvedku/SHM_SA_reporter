{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "\n",
    "# Load .env file from project root\n",
    "load_dotenv()\n",
    "\n",
    "# Read the MongoDB connection string (plain string, not JSON)\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "\n",
    "if not MONGODB_URI:\n",
    "    raise ValueError(\"‚ùå MONGODB_URI not found in .env\")\n",
    "\n",
    "client = pymongo.MongoClient(MONGODB_URI)\n",
    "\n",
    "db = client[\"prod\"]\n",
    "collection = db[\"PRJ-16\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10aa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime, UTC\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------\n",
    "CHUNK = 5000  # number of rows before flush\n",
    "PARQUET_DIR = Path(\"parquet_output\")\n",
    "\n",
    "# HUB definitions (unchanged)\n",
    "HUBS = {\n",
    "    \"hub1\": {\"A\": [\"A33\", \"A34\", \"A35\"],\n",
    "             \"S\": [\"S27\", \"S28\", \"S29\"],\n",
    "             \"T\": [\"T4\", \"T5\", \"T6\"]},\n",
    "\n",
    "    \"hub2\": {\"A\": [\"A30\"],\n",
    "             \"S\": [\"S7\",\"S8\",\"S9\",\"S10\",\"S11\",\"S12\",\"S13\",\"S14\"],\n",
    "             \"T\": [\"T1\"]},\n",
    "\n",
    "    \"hub3\": {\"A\": [\"A31\"],\n",
    "             \"S\": [\"S15\",\"S16\",\"S17\",\"S18\"],\n",
    "             \"T\": [\"T2\"]},\n",
    "\n",
    "    \"hub4\": {\"A\": [\"A32\"],\n",
    "             \"S\": [\"S19\",\"S20\",\"S21\",\"S22\",\"S23\",\"S24\",\"S25\",\"S26\"],\n",
    "             \"T\": [\"T3\"]},\n",
    "}\n",
    "\n",
    "ACCEL_ALL = (\n",
    "    HUBS[\"hub1\"][\"A\"]\n",
    "  + HUBS[\"hub2\"][\"A\"]\n",
    "  + HUBS[\"hub3\"][\"A\"]\n",
    "  + HUBS[\"hub4\"][\"A\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# SCHEMAS (exact same as your old parqueter)\n",
    "# ------------------------------------------------\n",
    "\n",
    "# ACCEL ALL\n",
    "accel_fields = [pa.field(\"datetime\", pa.timestamp(\"us\"))]\n",
    "for a in ACCEL_ALL:\n",
    "    accel_fields += [\n",
    "        pa.field(f\"{a}_t\", pa.float32()),\n",
    "        pa.field(f\"{a}_x\", pa.float32()),\n",
    "        pa.field(f\"{a}_y\", pa.float32()),\n",
    "        pa.field(f\"{a}_z\", pa.float32()),\n",
    "    ]\n",
    "ACCEL_ALL_SCHEMA = pa.schema(accel_fields)\n",
    "\n",
    "\n",
    "# FFT\n",
    "def make_fft_schema():\n",
    "    return pa.schema([\n",
    "        pa.field(\"datetime\", pa.timestamp(\"us\")),\n",
    "        pa.field(\"sensor_id\", pa.string()),\n",
    "        pa.field(\"fft_main_f\", pa.float32()),\n",
    "        pa.field(\"fft_main_a\", pa.float32()),\n",
    "        pa.field(\"fft_freqs\", pa.list_(pa.float32())),\n",
    "        pa.field(\"fft_amps\", pa.list_(pa.float32())),\n",
    "    ])\n",
    "\n",
    "\n",
    "# SST\n",
    "def make_sst_schema(hub):\n",
    "    fields = [pa.field(\"datetime\", pa.timestamp(\"us\"))]\n",
    "    for s in HUBS[hub][\"S\"]:\n",
    "        fields.append(pa.field(s, pa.float32()))\n",
    "    for t in HUBS[hub][\"T\"]:\n",
    "        fields.append(pa.field(t, pa.float32()))\n",
    "    return pa.schema(fields)\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# HELPERS\n",
    "# --------------------------------------------\n",
    "\n",
    "def safe_float(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def safe_list(items, key):\n",
    "    out = []\n",
    "    for it in items:\n",
    "        try:\n",
    "            out.append(float(it.get(key)))\n",
    "        except:\n",
    "            out.append(None)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5722a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_week_files(iso_year, iso_week):\n",
    "    week_str = f\"{iso_year}W{iso_week:02d}\"\n",
    "    base = PARQUET_DIR\n",
    "    base.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    files = {}\n",
    "\n",
    "    # accel_all\n",
    "    fn = base / f\"{week_str}_accel_all.parquet\"\n",
    "    pq.write_table(pa.Table.from_pylist([], ACCEL_ALL_SCHEMA), fn)\n",
    "    files[\"accel_all\"] = fn\n",
    "\n",
    "    # fft hubs\n",
    "    for hub in HUBS:\n",
    "        schema = make_fft_schema()\n",
    "        fn = base / f\"{week_str}_fft_{hub}.parquet\"\n",
    "        pq.write_table(pa.Table.from_pylist([], schema), fn)\n",
    "        files[f\"fft_{hub}\"] = fn\n",
    "\n",
    "    # sst hubs\n",
    "    for hub in HUBS:\n",
    "        schema = make_sst_schema(hub)\n",
    "        fn = base / f\"{week_str}_sst_{hub}.parquet\"\n",
    "        pq.write_table(pa.Table.from_pylist([], schema), fn)\n",
    "        files[f\"sst_{hub}\"] = fn\n",
    "\n",
    "    print(\"Created empty weekly parquet files:\")\n",
    "    for k, v in files.items():\n",
    "        print(\" -\", v)\n",
    "\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c39e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_rows_to_parquet(rows, schema, parquet_path):\n",
    "    if not rows:\n",
    "        return\n",
    "\n",
    "    tbl = pa.Table.from_pylist(rows, schema=schema)\n",
    "    duckdb.register(\"new_rows\", tbl)\n",
    "\n",
    "    # Ensure file exists before reading (EMPTY file already created)\n",
    "    duckdb.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP TABLE existing AS\n",
    "        SELECT * FROM parquet_scan('{parquet_path}', union_by_name=true)\n",
    "    \"\"\")\n",
    "\n",
    "    # Union existing + new\n",
    "    duckdb.execute(\"\"\"\n",
    "        CREATE OR REPLACE TEMP TABLE merged AS\n",
    "        SELECT * FROM existing\n",
    "        UNION ALL\n",
    "        SELECT * FROM new_rows\n",
    "    \"\"\")\n",
    "\n",
    "    # Overwrite parquet file\n",
    "    duckdb.execute(f\"\"\"\n",
    "        COPY merged TO '{parquet_path}' (FORMAT PARQUET, COMPRESSION ZSTD);\n",
    "    \"\"\")\n",
    "\n",
    "    duckdb.unregister(\"new_rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_week_cursor(cursor, iso_year, iso_week, weekly_files):\n",
    "    # buffers\n",
    "    accel_rows = []\n",
    "    fft_rows   = {hub: [] for hub in HUBS}\n",
    "    sst_rows   = {hub: [] for hub in HUBS}\n",
    "\n",
    "    for obj in cursor:\n",
    "        dt_raw = obj.get(\"time\", {}).get(\"datetime\")\n",
    "        if not dt_raw:\n",
    "            continue\n",
    "\n",
    "        dt = datetime.fromisoformat(dt_raw)\n",
    "\n",
    "        # ----------------------\n",
    "        # ACCEL\n",
    "        # ----------------------\n",
    "        ar = {\"datetime\": dt}\n",
    "        for a in ACCEL_ALL:\n",
    "            if a in obj:\n",
    "                ar[f\"{a}_t\"] = safe_float(obj[a].get(\"t\"))\n",
    "                ar[f\"{a}_x\"] = safe_float(obj[a].get(\"x\"))\n",
    "                ar[f\"{a}_y\"] = safe_float(obj[a].get(\"y\"))\n",
    "                ar[f\"{a}_z\"] = safe_float(obj[a].get(\"z\"))\n",
    "        accel_rows.append(ar)\n",
    "\n",
    "        if len(accel_rows) >= CHUNK:\n",
    "            append_rows_to_parquet(\n",
    "                accel_rows, ACCEL_ALL_SCHEMA,\n",
    "                weekly_files[\"accel_all\"]\n",
    "            )\n",
    "            accel_rows.clear()\n",
    "\n",
    "        # ----------------------\n",
    "        # HUBS ‚Üí SST + FFT\n",
    "        # ----------------------\n",
    "        for hub in HUBS:\n",
    "            # SST\n",
    "            sr = {\"datetime\": dt}\n",
    "            has_sst = False\n",
    "\n",
    "            for s in HUBS[hub][\"S\"]:\n",
    "                if s in obj:\n",
    "                    sr[s] = safe_float(obj[s].get(\"s\"))\n",
    "                    has_sst = True\n",
    "\n",
    "            for t in HUBS[hub][\"T\"]:\n",
    "                if t in obj:\n",
    "                    sr[t] = safe_float(obj[t].get(\"t\"))\n",
    "                    has_sst = True\n",
    "\n",
    "            if has_sst:\n",
    "                sst_rows[hub].append(sr)\n",
    "                if len(sst_rows[hub]) >= CHUNK:\n",
    "                    append_rows_to_parquet(\n",
    "                        sst_rows[hub], make_sst_schema(hub),\n",
    "                        weekly_files[f\"sst_{hub}\"]\n",
    "                    )\n",
    "                    sst_rows[hub].clear()\n",
    "\n",
    "            # FFT\n",
    "            for a in HUBS[hub][\"A\"]:\n",
    "                fft = obj.get(a, {}).get(\"fft\")\n",
    "                if fft:\n",
    "                    mn = fft.get(\"main\", {}) or {}\n",
    "                    sp = fft.get(\"spectrum\", []) or []\n",
    "                    fft_rows[hub].append({\n",
    "                        \"datetime\": dt,\n",
    "                        \"sensor_id\": a,\n",
    "                        \"fft_main_f\": safe_float(mn.get(\"f\")),\n",
    "                        \"fft_main_a\": safe_float(mn.get(\"a\")),\n",
    "                        \"fft_freqs\": safe_list(sp, \"f\"),\n",
    "                        \"fft_amps\": safe_list(sp, \"a\"),\n",
    "                    })\n",
    "                    if len(fft_rows[hub]) >= CHUNK:\n",
    "                        append_rows_to_parquet(\n",
    "                            fft_rows[hub],\n",
    "                            make_fft_schema(),\n",
    "                            weekly_files[f\"fft_{hub}\"]\n",
    "                        )\n",
    "                        fft_rows[hub].clear()\n",
    "\n",
    "    # ----------------------\n",
    "    # FINAL FLUSH\n",
    "    # ----------------------\n",
    "    if accel_rows:\n",
    "        append_rows_to_parquet(\n",
    "            accel_rows, ACCEL_ALL_SCHEMA,\n",
    "            weekly_files[\"accel_all\"]\n",
    "        )\n",
    "\n",
    "    for hub in HUBS:\n",
    "        if fft_rows[hub]:\n",
    "            append_rows_to_parquet(\n",
    "                fft_rows[hub], make_fft_schema(),\n",
    "                weekly_files[f\"fft_{hub}\"]\n",
    "            )\n",
    "        if sst_rows[hub]:\n",
    "            append_rows_to_parquet(\n",
    "                sst_rows[hub], make_sst_schema(hub),\n",
    "                weekly_files[f\"sst_{hub}\"]\n",
    "            )\n",
    "\n",
    "    print(\"‚úî Week processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_week(collection, week_start_dt, week_end_dt, iso_year, iso_week):\n",
    "    # convert to iso8601 for Mongo\n",
    "    start_iso = week_start_dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    end_iso   = week_end_dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    # create empty weekly parquets\n",
    "    weekly_files = create_empty_week_files(iso_year, iso_week)\n",
    "\n",
    "    print(\"\\nüîç Querying MongoDB...\")\n",
    "    cursor = collection.find(\n",
    "        {\"time.datetime\": {\"$gte\": start_iso, \"$lt\": end_iso}},\n",
    "        batch_size=5000\n",
    "    )\n",
    "\n",
    "    print(\"üöÄ Processing week...\")\n",
    "    process_week_cursor(cursor, iso_year, iso_week, weekly_files)\n",
    "\n",
    "    print(\"\\nüéâ Weekly parquet files ready.\")\n",
    "    return weekly_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d112d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Compute previous ISO week range (UTC safe) ===\n",
    "\n",
    "from datetime import datetime, date, timedelta, UTC\n",
    "\n",
    "# Current date in UTC\n",
    "today = datetime.now(UTC).date()\n",
    "\n",
    "# ISO year/week of today\n",
    "iso_year, iso_week, _ = today.isocalendar()\n",
    "\n",
    "iso_week -= 1\n",
    "\n",
    "\n",
    "# Previous ISO week\n",
    "prev_week = iso_week - 1\n",
    "prev_year = iso_year\n",
    "\n",
    "# Handle wrap to previous year\n",
    "if prev_week == 0:\n",
    "    prev_year -= 1\n",
    "    prev_week = date(prev_year, 12, 28).isocalendar()[1]\n",
    "\n",
    "# Start of previous week (Monday)\n",
    "start_prev_week = date.fromisocalendar(prev_year, prev_week, 1)\n",
    "\n",
    "# Start of current week (Monday)\n",
    "start_current_week = date.fromisocalendar(iso_year, iso_week, 1)\n",
    "\n",
    "# Convert to timezone-aware datetimes\n",
    "start_dt = datetime.combine(start_prev_week, datetime.min.time(), tzinfo=UTC)\n",
    "end_dt   = datetime.combine(start_current_week, datetime.min.time(), tzinfo=UTC)\n",
    "\n",
    "print(f\"Today:              {today}\")\n",
    "print(f\"Current ISO week:   {iso_year}-W{iso_week}\")\n",
    "print(f\"Previous ISO week:  {prev_year}-W{prev_week}\")\n",
    "print()\n",
    "print(f\"Week start:         {start_dt}\")\n",
    "print(f\"Week end:           {end_dt}  <-- start of current week\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_files = process_week(collection, start_dt, end_dt, prev_year, prev_week)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "NEW_DIR = Path(\"parquet_output\")\n",
    "OLD_DIR = Path(\"w48_old\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0185b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_parquets(folder: Path):\n",
    "    return sorted([p for p in folder.glob(\"*.parquet\")])\n",
    "\n",
    "old_files = list_parquets(OLD_DIR)\n",
    "new_files = list_parquets(NEW_DIR)\n",
    "\n",
    "print(\"OLD:\", len(old_files), \"files\")\n",
    "print(\"NEW:\", len(new_files), \"files\")\n",
    "\n",
    "print(\"\\nOLD FILES:\")\n",
    "for f in old_files:\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "print(\"\\nNEW FILES:\")\n",
    "for f in new_files:\n",
    "    print(\" -\", f.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parquet_info(path):\n",
    "    df = duckdb.sql(f\"DESCRIBE SELECT * FROM parquet_scan('{path}')\").df()\n",
    "    cnt = duckdb.sql(f\"SELECT COUNT(*) AS n FROM parquet_scan('{path}')\").df()[\"n\"][0]\n",
    "    return df, cnt\n",
    "\n",
    "comparison = []\n",
    "\n",
    "for old in old_files:\n",
    "    new = NEW_DIR / old.name\n",
    "    if not new.exists():\n",
    "        comparison.append({\n",
    "            \"file\": old.name,\n",
    "            \"status\": \"‚ùå missing in NEW\",\n",
    "            \"rows_old\": None,\n",
    "            \"rows_new\": None,\n",
    "            \"schema_match\": False\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    old_schema, old_rows = parquet_info(old)\n",
    "    new_schema, new_rows = parquet_info(new)\n",
    "\n",
    "    # Compare schema\n",
    "    schema_match = (\n",
    "        list(old_schema[\"column_name\"]) == list(new_schema[\"column_name\"]) and\n",
    "        list(old_schema[\"column_type\"]) == list(new_schema[\"column_type\"])\n",
    "    )\n",
    "\n",
    "    comparison.append({\n",
    "        \"file\": old.name,\n",
    "        \"status\": \"OK\" if (schema_match and old_rows == new_rows) else \"DIFF\",\n",
    "        \"rows_old\": old_rows,\n",
    "        \"rows_new\": new_rows,\n",
    "        \"schema_match\": schema_match\n",
    "    })\n",
    "\n",
    "print(pd.DataFrame(comparison))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01679e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.DataFrame(comparison)\n",
    "print(df_compare)\n",
    "\n",
    "print(\"\\nFILES FULLY MATCHING:\")\n",
    "print(df_compare[df_compare[\"status\"] == \"OK\"][\"file\"].tolist())\n",
    "\n",
    "print(\"\\nFILES DIFFERING:\")\n",
    "print(df_compare[df_compare[\"status\"] != \"OK\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"2025W48_sst_hub2.parquet\"   # or fft_hub2\n",
    "old_path = OLD_DIR / FILE\n",
    "new_path = NEW_DIR / FILE\n",
    "\n",
    "diff = duckdb.sql(f\"\"\"\n",
    "WITH old AS (\n",
    "    SELECT * FROM parquet_scan('{old_path}')\n",
    "),\n",
    "new AS (\n",
    "    SELECT * FROM parquet_scan('{new_path}')\n",
    ")\n",
    "SELECT \n",
    "    new.datetime AS new_dt,\n",
    "    old.datetime AS old_dt,\n",
    "    *\n",
    "FROM new\n",
    "FULL OUTER JOIN old USING (datetime)\n",
    "WHERE old.datetime IS NULL OR new.datetime IS NULL\n",
    "ORDER BY new_dt NULLS LAST, old_dt NULLS LAST\n",
    "\"\"\").df()\n",
    "\n",
    "diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"2025W48_sst_hub2.parquet\"\n",
    "old_path = OLD_DIR / FILE\n",
    "new_path = NEW_DIR / FILE\n",
    "\n",
    "# Extract unique timestamps from diff\n",
    "missing_ts = diff[\"new_dt\"].dropna().unique().tolist()\n",
    "\n",
    "print(\"Missing timestamps detected:\", missing_ts)\n",
    "\n",
    "results = []\n",
    "\n",
    "for ts in missing_ts:\n",
    "    ts_str = ts.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]  # microsecond trimming if needed\n",
    "\n",
    "    old_count = duckdb.sql(f\"\"\"\n",
    "        SELECT COUNT(*) AS n\n",
    "        FROM parquet_scan('{old_path}')\n",
    "        WHERE datetime = '{ts_str}'\n",
    "    \"\"\").df()[\"n\"][0]\n",
    "\n",
    "    new_count = duckdb.sql(f\"\"\"\n",
    "        SELECT COUNT(*) AS n\n",
    "        FROM parquet_scan('{new_path}')\n",
    "        WHERE datetime = '{ts_str}'\n",
    "    \"\"\").df()[\"n\"][0]\n",
    "\n",
    "    results.append({\n",
    "        \"timestamp\": ts_str,\n",
    "        \"exists_in_old\": old_count,\n",
    "        \"exists_in_new\": new_count,\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39857e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in sorted(NEW_DIR.glob(\"*.parquet\")):\n",
    "    print(\"==========================================\")\n",
    "    print(\"FILE:\", p.name)\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    cols = duckdb.sql(f\"\"\"\n",
    "        DESCRIBE SELECT * FROM parquet_scan('{p}', union_by_name=true)\n",
    "    \"\"\").df()\n",
    "\n",
    "    print(cols.to_string(index=False))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rep_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
